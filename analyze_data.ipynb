{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Correlation computation\n",
    "This script takes data preprocessed by preprocess_data script and calculates Pearson and Mutual Information correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "from pyspark import SparkContext, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum is your nr_of_cores x 2 \n",
    "# (or the number of logical processes).\n",
    "# Picking more or less will change the running time \n",
    "# (more doesn't mean that it's going to be faster).\n",
    "THREADS_TO_USE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC = SparkContext(f'local[{THREADS_TO_USE}]')\n",
    "print(SC.version)\n",
    "SQL_CONTEXT = SQLContext(SC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_corr(vector_1, vector_2):\n",
    "    v1_squared_sum = sum([x**2 for x in vector_1])\n",
    "    v2_squared_sum = sum([x**2 for x in vector_2])\n",
    "    numerator = sum([x*y for (x,y) in zip(vector_1, vector_2)])\n",
    "    denominator = (v1_squared_sum**(1/2)) * (v2_squared_sum**(1/2))\n",
    "    if denominator == 0: \n",
    "        return 0\n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(vector_a, vector_b):\n",
    "    n = len(vector_a)\n",
    "    probas_a = Counter(vector_a)\n",
    "    probas_b = Counter(vector_b)\n",
    "    probas_ab = Counter(zip(vector_a, vector_b))\n",
    "    for a in probas_a:\n",
    "        probas_a[a] /= n\n",
    "    for b in probas_b:\n",
    "        probas_b[b] /= n\n",
    "    for ab in probas_ab:\n",
    "        probas_ab[ab] /= n\n",
    "    \n",
    "    mi = 0\n",
    "    for x in probas_a:\n",
    "        p_a = probas_a[x]\n",
    "        for y in probas_b:\n",
    "            p_b, p_ab = probas_b[y], probas_ab[(x,y)]\n",
    "            if p_ab == 0: \n",
    "                continue\n",
    "            mi += (p_ab)*(log(p_ab) - log(p_a*p_b))\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_groups(group_a, group_b, correlation_function):\n",
    "    \"\"\" Each group is a list of tuples. Each tuple is \n",
    "    in a format of ((Stock, Value), idx).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Stock\n",
    "        String with the name of the stock.\n",
    "    Value\n",
    "        List with the values of that stock.\n",
    "    idx\n",
    "        Unique index of that stock.\n",
    "    \"\"\"\n",
    "    print(group_a[0], group_b[0])\n",
    "    \n",
    "    if group_a[0] == group_b[0]:\n",
    "        size_of_groups = len(group_a[1])\n",
    "                             \n",
    "        for i in range(size_of_groups):\n",
    "            stock_a = group_a[1][i][0]\n",
    "            for j in range(i+1, size_of_groups):\n",
    "                stock_b = group_b[1][j][0]\n",
    "                if stock_a[0].rsplit('-', 1)[0] != stock_b[0].rsplit('-', 1)[0]:\n",
    "                    yield (\n",
    "                        correlation_function(stock_a[1], stock_b[1]), \n",
    "                        (stock_a[0], stock_b[0]),\n",
    "                    )\n",
    "    else:\n",
    "        for zip_tuple_a in group_a[1]:\n",
    "            stock_a = zip_tuple_a[0]\n",
    "            for zip_tuple_b in group_b[1]:\n",
    "                stock_b = zip_tuple_b[0]\n",
    "                if stock_a[0].rsplit('-', 1)[0] != stock_b[0].rsplit('-', 1)[0]:\n",
    "                    yield (\n",
    "                        correlation_function(stock_a[1], stock_b[1]), \n",
    "                        (stock_a[0], stock_b[0]),\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(rdd, number_groups, correlation):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    rdd\n",
    "        RDD in a format of [(StockA, ValueA), (StockB, ValueB), ...]\n",
    "    number_groups\n",
    "        Number if batches to divide the data into.\n",
    "        It's best to pick a number that's bigger than nr of threads \n",
    "        you are using, but not much more.\n",
    "    \"\"\"\n",
    "    # Assign indices to each stock.\n",
    "    rdd = rdd.zipWithIndex()\n",
    "    # [((StockA, ValueA), idx=0), ((StockB, ValueB), idx=1), ...]\n",
    "    \n",
    "    # Divide stocks into buckets.\n",
    "    rdd = rdd.groupBy(\n",
    "        lambda zipTuple: zipTuple[1] % number_groups\n",
    "    ).mapValues(tuple)\n",
    "    # [(0, (zip tuples where idx%batch_size==0)), \n",
    "    #  (1, (zip tuples where idx%batch_size==1)), \n",
    "    # ...]\n",
    "    \n",
    "    # Create combinations of all possible buckets.\n",
    "    rdd = rdd.cartesian(rdd)\n",
    "    # [((0, (zip tuple)), (0, (zip tuple))), \n",
    "    #  ((0, (zip tuple)), (1, (zip tuple))), \n",
    "    #  ..., \n",
    "    #  ((1, (zip tuple)), (0, (zip tuple))),\n",
    "    #  ...] \n",
    "    # So all combinations including the useless ones (1,0), (2,0), (2,1)...\n",
    "    \n",
    "    # If we have a correlation between buckets (1,2), then filter out (2,1)\n",
    "    # We need to keep (0,0), (1,1), ..., because we want correlations within the group\n",
    "    rdd = rdd.filter(lambda x: x[0][0] <= x[1][0])\n",
    "    # [((0, (zip tuple)), (1, (zip tuple))),\n",
    "    # ..., \n",
    "    # ((1, (zip tuple)), (2, (zip tuple))), \n",
    "    # ...]\n",
    "    \n",
    "    # We need to correlate every stock inside each \n",
    "    # (zip tuple)=((StockA, ValueB), idx).\n",
    "    correlations = rdd.flatMap(\n",
    "        lambda x: compare_groups(x[0], x[1], correlation)\n",
    "    )\n",
    "\n",
    "    return correlations.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, prefix, column):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df\n",
    "        The dataframe that was loaded from pearson or MI\n",
    "    prefix\n",
    "        p or MI depending on which correlation.\n",
    "    column\n",
    "        Column name to use for correlation.\n",
    "    \"\"\"\n",
    "    rdd = df.select(\n",
    "        'stock', f'{prefix}_{column}'\n",
    "    ).rdd.map(tuple).groupByKey().sortByKey()\n",
    "    # [(StockA, ValuesA), (StockB, ValuesB)]\n",
    "    \n",
    "    rdd = rdd.map(lambda x: (f'{x[0]}-{column}', x[1]))\n",
    "    # [(StockA_opening_price, ValuesA), (StockB_opening_price, ValuesB)]\n",
    "\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlations(parquet_name, prefix, correlation_func):\n",
    "    \"\"\" Gets correlations for specified data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    parquet_name\n",
    "        Name of the file were the data is located.\n",
    "    prefix\n",
    "        p or MI depending whether it's Pearson or Mutual information.\n",
    "    correlation_func\n",
    "        `pearson_corr` or `mutual_information`.\n",
    "    \"\"\"\n",
    "    \n",
    "    columns_for_corr = ['opening_price', 'highest_price', 'lowest_price']\n",
    "    \n",
    "    stock_data = SQL_CONTEXT.read.parquet(parquet_name)\n",
    "    all_stocks_data = [\n",
    "        prepare_data(stock_data, prefix, col) \n",
    "        for col \n",
    "        in columns_for_corr\n",
    "    ]\n",
    "    stock_rdd = SC.union(all_stocks_data).sortByKey()\n",
    "    \n",
    "    print(f'We have {stock_rdd.count()} vectors '\n",
    "          f'with {len(stock_rdd.take(1)[0][1])} dimensions each.')\n",
    "    \n",
    "    # Because some workers will get less work it's good\n",
    "    # to divide data into THREADS_TO_USE*x batches\n",
    "    # instead if THREADS_TO_USE batches\n",
    "    result = compute_correlations(\n",
    "        stock_rdd, THREADS_TO_USE*2, correlation_func)\n",
    "    \n",
    "    result.sort(reverse=True, key=lambda tup: abs(tup[0]))\n",
    "    return result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pearson_correlated_data = get_correlations(\n",
    "    'pearson_corr.parquet', 'p', pearson_corr,\n",
    ")\n",
    "\n",
    "print('Top 10 correlations according to Pearson: ')\n",
    "for data_point in pearson_correlated_data:\n",
    "    print(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MI_correlated_data = get_correlations(\n",
    "    'MI_corr.parquet', 'MI', mutual_information,\n",
    ")\n",
    "\n",
    "print('Top 10 correlations according to Mutual Information: ')\n",
    "for data_point in MI_correlated_data:\n",
    "    print(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
