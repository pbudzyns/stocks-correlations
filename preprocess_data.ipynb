{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Data proprocessing\n",
    "We are preprocessing here 2020 stock data.\n",
    "This script creates 4 folders:\n",
    "* extrapolated_data.parquet - data preprocesses up to and including extrapolation\n",
    "* hourly_data.parquet - data preprocessed with hourly frequency (made out of extrapolated_data.parquet)\n",
    "* pearson_corr.parquet - data preprocessed specifically for usage with pearson correlation\n",
    "* MI_corr.parquet - data preprocessed specifically for usage with MI correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from operator import attrgetter\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "SC = SparkContext()\n",
    "print(SC.version)\n",
    "SQL_CONTEXT = SQLContext(SC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_LOCATION = \"./2020/*.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCKS_SCHEMA = StructType([\n",
    "    StructField('date', StringType(), False),\n",
    "    StructField('time', StringType(), False),\n",
    "    StructField('opening_price', DoubleType(), False),\n",
    "    StructField('highest_price', DoubleType(), False),\n",
    "    StructField('lowest_price', DoubleType(), False),\n",
    "    StructField('closing_price', DoubleType(), False),\n",
    "    StructField('sum_of_transactions', DoubleType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_into_df(file_location):\n",
    "    headers = (\n",
    "        'date', 'time', 'opening_price', \n",
    "        'highest_price', 'lowest_price', \n",
    "        'closing_price', 'sum_of_transactions',\n",
    "    )\n",
    "    \n",
    "    df = (\n",
    "        SQL_CONTEXT\n",
    "        .read\n",
    "        .format('csv')\n",
    "        .options(\n",
    "            header=False, delimiter=',', \n",
    "            inferSchema=True, schema=STOCKS_SCHEMA,\n",
    "        )\n",
    "        .load(file_location)\n",
    "        .toDF(*headers)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_name(df):\n",
    "    stock_name_regex = '(.*\\/[0-9]*_)|(_NoExpiry.txt)'\n",
    "    df = df.withColumn(\"filename\", fn.input_file_name())\n",
    "    df = df.withColumn(\n",
    "        \"stock\", \n",
    "        fn.split(fn.col('filename'), stock_name_regex)[1],\n",
    "    )\n",
    "    df = df.drop('filename')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(df):\n",
    "    df = df.withColumn(\n",
    "        'date_time',\n",
    "        fn.concat(fn.col('date'),fn.lit(' '), fn.col('time')),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        'timestamp', \n",
    "        fn.to_timestamp(fn.col(\"date_time\"),\"MM/dd/yyyy HH:mm\"),\n",
    "    )\n",
    "    columns_to_drop = ['time', 'date_time']\n",
    "    df = df.drop(*columns_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_months(df, no_months):\n",
    "    \"\"\"Returns a dataframe where stocks have at least \n",
    "    data of 'no_months' different months.\n",
    "    \"\"\"\n",
    "    group_by_stock_data = df.groupBy([\"stock\"])\n",
    "    df_no_months = group_by_stock_data.agg(\n",
    "        fn.countDistinct(fn.month('timestamp')).alias('no_months')\n",
    "    )\n",
    "    df_all_months = df_no_months.filter(df_no_months['no_months'] == no_months)\n",
    "    df = df.join(df_all_months, on=['stock'], how='inner').drop('no_months')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_hours(df, start, end):\n",
    "    return df.filter(\n",
    "        (\n",
    "            (fn.hour(df['timestamp']) >= start) \n",
    "            & (fn.hour(df['timestamp']) <= (end - 1))\n",
    "        ) | (\n",
    "            (fn.minute(df['timestamp']) == 0) \n",
    "            & (fn.hour(df['timestamp']) == end)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_data(df):\n",
    "    \"\"\" Returns a dataframe where duplicate \n",
    "    timestamps are aggregrated. We don't take \n",
    "    sum_of_transactions anymore, because \n",
    "    we don't use it later in the data.\n",
    "    \"\"\"\n",
    "    group_data = df.groupBy([\"stock\",\"timestamp\",\"date\"])\n",
    "    df_dedup = group_data.agg(\n",
    "        fn.avg('opening_price').alias('opening_price'),\n",
    "        fn.max('highest_price').alias('highest_price'),\n",
    "        fn.min('lowest_price').alias('lowest_price'),\n",
    "        fn.avg('closing_price').alias('closing_price')\n",
    "    ).orderBy([\"stock\",\"timestamp\"])\n",
    "    \n",
    "    return df_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_frequency(df, freq):\n",
    "    \"\"\" Returns a dataframe where the stocks that have \n",
    "    less than some number of rows are filtered out.\n",
    "    \"\"\"\n",
    "    group_by_stock_data = df.groupBy([\"stock\"])\n",
    "    \n",
    "    df_nr_of_records = group_by_stock_data.agg(\n",
    "        fn.count('stock').alias('nr_of_records')\n",
    "    )\n",
    "    \n",
    "    df_eligible_stocks = df_nr_of_records.filter(\n",
    "        df_nr_of_records['nr_of_records'] > freq\n",
    "    )\n",
    "    \n",
    "    df = df.join(\n",
    "        df_eligible_stocks, on=['stock'], how='inner',\n",
    "    ).drop('nr_of_records')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(schema, freq, timestamp_col = \"timestamp\", **kwargs):\n",
    "    \"\"\" Returns a function that resamples at a certain \n",
    "    frequency and interpolates the data.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    schema -- dataframe schema\n",
    "    freq -- frequency at which need to be resampled\n",
    "    timestamp_col -- column used for resampling/interpolation (default 'timestamp')\n",
    "    \"\"\"\n",
    "    @fn.pandas_udf(\n",
    "        StructType(sorted(schema, key=attrgetter(\"name\"))), \n",
    "        fn.PandasUDFType.GROUPED_MAP,\n",
    "    )\n",
    "    \n",
    "    def _(pdf):\n",
    "        pdf.set_index(timestamp_col, inplace=True)\n",
    "        pdf = pdf.resample(freq).interpolate()\n",
    "        pdf.ffill(inplace=True)\n",
    "        pdf.reset_index(drop=False, inplace=True)\n",
    "        pdf.sort_index(axis=1, inplace=True)\n",
    "        return pdf\n",
    "    \n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(df, freq_range='60S'):\n",
    "    \"\"\" Returns a dataframe where each stock, \n",
    "    at the available dates, is resampled at 'freq_range' \n",
    "    and interpolation is used to fill missing values.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .groupBy([\"stock\", \"date\"])\n",
    "        .apply(resample(df.schema, freq_range))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_that_have_enough_stocks(df, limit):\n",
    "    diff_stocks_per_day_df = (\n",
    "        df\n",
    "        .groupBy([\"date\"])\n",
    "        .agg(fn.countDistinct('stock').alias('diff_stocks'))\n",
    "        .orderBy(\"date\")\n",
    "    )\n",
    "    enough_stocks_df = diff_stocks_per_day_df.filter(\n",
    "        diff_stocks_per_day_df['diff_stocks'] >= limit\n",
    "    )\n",
    "    return enough_stocks_df.select(\"date\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_range(days, start_hour, end_hour, freq, index_name='timestamp'):\n",
    "    \"\"\" Returns the range as DatetimeIndex.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    days -- days of the period as pandas df\n",
    "    start_time -- time at which a working day starts as string (using 24h)\n",
    "    end_time -- time at which a working day ends as string (using 24h)\n",
    "    freq -- frequency at which need to be resampled\n",
    "    index_name -- name of the index (default 'timestamp')\n",
    "    \"\"\"\n",
    "    days['date'] = pd.to_datetime(days['date'])\n",
    "    index_date = pd.Series(days['date'])\n",
    "    index_time = pd.date_range(start_hour, end_hour, freq=freq)\n",
    "    index_time = pd.Series(index_time.time)\n",
    "\n",
    "    index = index_date.apply(\n",
    "        lambda d: index_time.apply(\n",
    "            lambda t: datetime.combine(d, t)\n",
    "            )\n",
    "        ).unstack().sort_values().reset_index(drop=True)\n",
    "    \n",
    "    return pd.DatetimeIndex(index, name='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_fn(schema, working_range, timestamp_col = \"timestamp\", **kwargs):\n",
    "    \"\"\" Returns a function that aligns time between stocks \n",
    "    and 'extrapolates' the data by filling missing values \n",
    "    by the nearest.\n",
    "        \n",
    "    Keyword arguments:\n",
    "    schema -- dataframe schema\n",
    "    working_range -- range of timestamps for which values need to be generated for each stock\n",
    "    timestamp_col -- column used for resampling/extrapolation (default 'timestamp')\n",
    "    \"\"\"\n",
    "    @fn.pandas_udf(\n",
    "        StructType(sorted(schema, key=attrgetter(\"name\"))), \n",
    "        fn.PandasUDFType.GROUPED_MAP,\n",
    "    )\n",
    "    def _(pdf):\n",
    "        pdf.set_index(timestamp_col, inplace=True)\n",
    "        pdf.sort_index(inplace=True)\n",
    "        pdf = pdf.reindex(working_range, method='nearest')\n",
    "        pdf.reset_index(drop=False, inplace=True)\n",
    "        pdf.sort_index(axis=1, inplace=True)\n",
    "        return pdf\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate(df, min_no_of_stocks, start_hour, end_hour):\n",
    "    \"\"\" Returns a dataframe where the stocks are time aligned\n",
    "    and missing values are filled by the nearest available value.\n",
    "    \"\"\"\n",
    "    days = get_days_that_have_enough_stocks(df, min_no_of_stocks)\n",
    "    day_index = get_day_range(\n",
    "        days, start_hour=start_hour, \n",
    "        end_hour=end_hour, freq='1min', \n",
    "        index_name='timestamp',\n",
    "    )\n",
    "    return df.groupBy([\"stock\"]).apply(extrapolate_fn(df.schema, day_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_data_to_hourly(df):\n",
    "    \"\"\" Returns a dataframe where stock data is aggregate by hour.\n",
    "    \"\"\"\n",
    "    df_with_hours = df.withColumn(\n",
    "        'timestamp', \n",
    "        fn.date_trunc('Hour', df['timestamp']),\n",
    "    )\n",
    "    group_data = df_with_hours.groupBy([\"stock\",\"timestamp\"])\n",
    "    hourly_df = group_data.agg(\n",
    "            fn.first('opening_price').alias('opening_price'),\n",
    "            fn.max('highest_price').alias('highest_price'),\n",
    "            fn.min('lowest_price').alias('lowest_price'),\n",
    "            fn.last('closing_price').alias('closing_price'),\n",
    "    ).orderBy([\"stock\",\"timestamp\"])\n",
    "    return hourly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_for_MI(df, value):\n",
    "    window = Window.partitionBy(\n",
    "        ['stock', fn.month('timestamp')]\n",
    "    ).orderBy('timestamp')\n",
    "    \n",
    "    prev_value = \"prev_value_\" + value\n",
    "    df = df.withColumn(prev_value, fn.lag(df[value]).over(window))\n",
    "    df = df.withColumn(\n",
    "        \"MI_\" + value, \n",
    "        (fn\n",
    "         .when(fn.isnull(df[prev_value]) | (df[prev_value] == df[value]), 0)\n",
    "         .when(df[value] > df[prev_value], 1)\n",
    "         .otherwise(-1)\n",
    "        ),\n",
    "    )\n",
    "    df = df.drop(prev_value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_for_MI(df,no_buckets,value):\n",
    "    discretizer = QuantileDiscretizer(\n",
    "        numBuckets=no_buckets, \n",
    "        inputCol=value,\n",
    "        outputCol=\"MI_\" + value,\n",
    "    )\n",
    "    return discretizer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_MI(df, values,no_buckets):\n",
    "    for value in values:\n",
    "        df = discretize_for_MI(df, no_buckets, value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pearson(df, values):\n",
    "    group_data = df.groupBy([\"stock\"])\n",
    "    averages = [fn.avg(value).alias('avg_'+value) for value in values]\n",
    "    df_avg = group_data.agg(*averages)\n",
    "    \n",
    "    df = df.join(df_avg, on=['stock'], how='inner')\n",
    "    for value in values:\n",
    "        df = df.withColumn(\"p_\" + value, df[value] - df['avg_'+value])\n",
    "    \n",
    "    drop_columns = ['avg_' + value for value in values]\n",
    "    df = df.drop(*drop_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('extrapolated_data.parquet'):\n",
    "    df = read_files_into_df(DATA_LOCATION)\n",
    "    df = get_stock_name(df)\n",
    "    df = get_timestamp(df)\n",
    "\n",
    "    df = filter_months(df, 4)\n",
    "    df = filter_hours(df, 9, 17)\n",
    "    df = deduplicate_data(df)\n",
    "\n",
    "    # 12000 is around the minimum nr of records you \n",
    "    # should have for ~22 working days per month, \n",
    "    # 3 months + 10 more days in April, 8h a day, 5min freq interval\n",
    "    # after this filtering we have around 666 different stocks left.\n",
    "    df = filter_frequency(df, 12000)\n",
    "\n",
    "    # Conda install -c conda-forge pyarrow=0.13.\n",
    "    df = interpolate(df)\n",
    "    \n",
    "    # We checked how many different stocks each day has and \n",
    "    # decided that taking 650 as a minimum stock limit is the best\n",
    "    df = extrapolate(df, 650, '09:00:00', '17:00:00')\n",
    "    \n",
    "    df.write.parquet('extrapolated_data.parquet')\n",
    "    \n",
    "df = SQL_CONTEXT.read.parquet('extrapolated_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('hourly_data.parquet'):\n",
    "    df = turn_data_to_hourly(df)\n",
    "    df.write.parquet('hourly_data.parquet')\n",
    "    \n",
    "df = SQL_CONTEXT.read.parquet('hourly_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_corr = [\n",
    "    'opening_price', 'highest_price', \n",
    "    'lowest_price', 'closing_price',\n",
    "]\n",
    "\n",
    "if not os.path.isdir('pearson_corr.parquet'):   \n",
    "    pearson_df = preprocess_pearson(df, columns_for_corr)\n",
    "    pearson_df.write.parquet('pearson_corr.parquet')\n",
    "    \n",
    "if not os.path.isdir('MI_corr.parquet'):   \n",
    "    # 200 bins is the optimal no of bins \n",
    "    MI_df = preprocess_MI(df, columns_for_corr, 200)\n",
    "    MI_df.write.parquet('MI_corr.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
